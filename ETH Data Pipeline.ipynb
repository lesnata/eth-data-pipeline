{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"N5xKgd_JDXPf"},"source":["%%capture\n","### (1) Install dependencies and restart runtime\n","import os\n","\n","!pip install --upgrade jsonschema==4.4.0\n","!pip install --upgrade web3==5.26.0\n","\n","os.kill(os.getpid(), 9)  # Hack to restart the runtime after install\n","# This will throw an error"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### (2) Verify installation\n","from web3 import Web3"],"metadata":{"id":"obrbJ6Flomiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrCPM7y-HwHx"},"source":["### (3) Define constants\n","\n","# Infura free plan API key. Don't abuse this please!\n","NODE_ENDPOINT = \"https://mainnet.infura.io/v3/SECRET\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8FWDaX-ITGl"},"source":["### (4) Authenticate with Google Cloud Platform to run BigQuery queries\n","from google.colab import auth\n","auth.authenticate_user()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ys1_tg3wYI7u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3zLsNKX2s9o"},"source":["# DESCRIPTION"]},{"cell_type":"markdown","metadata":{"id":"lkI6aePz2JB7"},"source":["A data pipeline for enriching and summarizing ethereum token transfer data one day at a time.\n","\n","1. Summatizing data from the public BigQuery table `bigquery-public-data.crypto_ethereum.token_transfers`.\n","  - Count the number of transactions for each `token_address` and save top 100\n","1. Enriching the data with information from the blockchain using the provided Infura node endpoint\n","  - Append token `name` and `symbol`\n","1. Processing one day at a time\n","1. Each execution of the pipeline triggered by running the Python function `run_batch_job` (see below)\n","1. Storing the result of each run in a BigQuery table \n","\n","\n","| event_date | token_name | token_symbol | token_address | number_of_transfers |\n","|---|---|---|---|---|\n","| 2022-01-01 | USD Coin | USDC | 0x123... | 1000 | \n","| 2022-01-01 | Aave | AAVE | 0x456... | 800 | \n","| 2022-01-01 | Wrapped Ether | WETH | 0x789... | 600 | \n","\n","Extra:\n"," - The pipeline uses as few Infura lookups as possible\n"," - You may not use state in memory between pipeline runs (variables in Python)\n","\n","for dates 2022-01-01 -> 2022-01-03 (three days)"]},{"cell_type":"code","source":["from google.cloud import bigquery\n","\n","PROJECT = 'PROJECT_NAME'\n","TARGET_DATASET = 'token_dataset'\n","TARGET_TABLE = 'token_transfer'\n","TARGET_GLOSSARY = 'token_glossary'\n","\n","BQ = bigquery.Client(project=PROJECT)\n","\n","w3 = Web3(Web3.HTTPProvider(NODE_ENDPOINT))\n","ABI = [\n","  # One list entry for each contract function\n","  {\n","    \"constant\": True,\n","    \"inputs\": [],\n","    \"name\": \"name\",\n","    \"outputs\": [\n","      {\n","        \"name\": \"name\",\n","        \"type\": \"string\"\n","      },\n","    ],\n","    \"type\": \"function\"\n","  },\n","    {\n","    \"constant\": True,\n","    \"inputs\": [],\n","    \"name\": \"symbol\",\n","    \"outputs\": [\n","      {\n","        \"symbol\": \"symbol\",\n","        \"type\": \"string\"\n","      },\n","    ],\n","    \"type\": \"function\"\n","  }\n","]\n","\n","schema = [\n","    bigquery.SchemaField(\"token_address\", \"string\", mode=\"REQUIRED\"),\n","    bigquery.SchemaField(\"token_name\", \"string\", mode=\"REQUIRED\"),\n","    bigquery.SchemaField(\"token_symbol\", \"string\", mode=\"REQUIRED\"),\n","]\n","\n","table_glossary = bigquery.Table(f\"{PROJECT}.{TARGET_DATASET}.{TARGET_GLOSSARY}\", schema=schema)\n","#TODO check if table doesn't exist\n","table_create_glossary = BQ.create_table(table_glossary)\n"],"metadata":{"id":"hlYrA612qUDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["schema = [\n","    bigquery.SchemaField(\"event_date\", \"DATE\"),\n","    bigquery.SchemaField(\"token_address\", \"STRING\"),\n","    bigquery.SchemaField(\"number_of_transfers\", \"INTEGER\"),\n","    bigquery.SchemaField(\"token_name\", \"STRING\"),\n","    bigquery.SchemaField(\"token_symbol\", \"STRING\"),]\n","\n","#TODO check if table doesn't exist\n","table = bigquery.Table(f\"{PROJECT}.{TARGET_DATASET}.{TARGET_TABLE}\", schema=schema)\n","table_create = BQ.create_table(table) "],"metadata":{"id":"lgShxsDkqXCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.cloud import bigquery\n","import pandas as pd\n","\n","LIMIT_NUMBER = 10\n","\n","def enrich_token_data(token_address: str) -> str:\n","  contract = w3.eth.contract(w3.toChecksumAddress(token_address), abi=ABI)\n","  #TODO check data type match\n","  try:\n","    name = contract.functions.name().call()\n","    symbol = contract.functions.symbol().call()\n","  except:\n","    print(\"Errors with w3.eth.contract\")\n","  return name, symbol\n","\n","def run_batch_job(date: str):\n","  #TODO implement check if user has upgraded tier account. This one is for upgraded\n","  eth_df = BQ.query(f\"\"\"select CAST('{date}' AS DATE) as event_date, token_address, CAST(count(*) AS INT) as number_of_transfers \n","                        from bigquery-public-data.crypto_ethereum.token_transfers \n","                        where date(block_timestamp) = '{date}' group by token_address \n","                        order by number_of_transfers desc limit {LIMIT_NUMBER};\"\"\").to_dataframe()\n","  token_glossary = BQ.query(f\"\"\"select * from {PROJECT}.{TARGET_DATASET}.token_glossary\"\"\").to_dataframe()\n","\n","  get_target_token_table = BQ.get_table(f\"{PROJECT}.{TARGET_DATASET}.{TARGET_TABLE}\")\n","  selected_fields = get_target_token_table.schema[:3]\n","  target_token_rows = BQ.list_rows(get_target_token_table, selected_fields=selected_fields)\n","  existing_rows = [row.values() for row in target_token_rows]\n","\n","  # Filter the rows to insert\n","  new_rows = [tuple(row) for row in eth_df.values if tuple(row) not in existing_rows]\n","\n","  # Insert the new rows into the table\n","  if new_rows:\n","    for i, r in enumerate(new_rows):\n","      # Checking if data in glossary table exists\n","      if r[1] in token_glossary.token_address.values:\n","        print(f\"Token address in Glossary table found\")\n","        name = token_glossary.loc[token_glossary.token_address == r[1], 'token_name'].values[0]\n","        symbol = token_glossary.loc[token_glossary.token_address == r[1], 'token_symbol'].values[0]\n","\n","      else:\n","        print(f\"Token address in glossary table is not found\")\n","        name, symbol = enrich_token_data(r[1])\n","        new_glossary_rows = tuple((r[1], name, symbol))\n","        result = BQ.insert_rows(table_glossary, [new_glossary_rows])\n","        print(f\"New token info for -- {name} -- inserted in the glossary\")\n","\n","      row_values = list(r)\n","      row_values.append(name)  \n","      row_values.append(symbol)\n","      new_rows[i] = tuple(row_values)\n","\n","    result = BQ.insert_rows(table, new_rows)\n","    return print(f\"{TARGET_TABLE} successfully updated\")\n","  else:\n","    print('No new rows to insert')\n","\n","\n","DATE = '2022-01-01'\n","run_batch_job(DATE)\n","# run_batch_job(run_date_2)\n","# run_batch_job(run_date_3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ynh3jzkirIhO","executionInfo":{"status":"ok","timestamp":1678659482540,"user_tz":0,"elapsed":15578,"user":{"displayName":"Natalie Lesyk","userId":"11779296607346774529"}},"outputId":"f87ea644-aeb7-4887-bb99-791a0df044a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Token address in glossary table is not found\n","New token info for -- GoldHunter -- inserted in the glossary\n","Token address in glossary table is not found\n","New token info for -- Phanta Bear -- inserted in the glossary\n","Token address in glossary table is not found\n","New token info for -- SHIBA INU -- inserted in the glossary\n","Token address in glossary table is not found\n","New token info for -- Matic Token -- inserted in the glossary\n","Token address in glossary table is not found\n","New token info for -- SOS -- inserted in the glossary\n","token_transfer successfully updated\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Wjqni2rwrIj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hezKMd70rIqJ"},"execution_count":null,"outputs":[]}]}